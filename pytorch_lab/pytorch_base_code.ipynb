{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 元素相乘\n",
    "该操作又称作 \"哈达玛积\", 简单来说就是 tensor 元素逐个相乘。这个操作，是通过 * 也就是常规的乘号操作符定义的操作结果。torch.mul 是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 4, 10, 18]), tensor([ 4, 10, 18]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 元素相乘，不是点乘\n",
    "def element_by_element():\n",
    "    \n",
    "    x = torch.tensor([1, 2, 3])\n",
    "    y = torch.tensor([4, 5, 6])\n",
    "    \n",
    "    return x * y, torch.mul(x, y)\n",
    "\n",
    "print (element_by_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量点乘\n",
    "向量相乘 每个元素按照列相乘之后再统一相加\n",
    "x(i) * y(i) + x(i+1) * y(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8., 9.]])\n",
      "tensor([2., 2.])\n",
      "tensor([10., 14., 18., 22., 26.])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "# 元素点乘：向量 * 向量  = 标量\n",
    "def vec_dot_product():\n",
    "    \n",
    "    x = torch.tensor([1, 2, 3])\n",
    "    y = torch.tensor([4, 5, 6])\n",
    "    \n",
    "    return torch.matmul(x, y)\n",
    "# vec_dot_product()\n",
    "\n",
    "# 点乘：矩阵 * 向量\n",
    "def vec_dot_product1():\n",
    "    \n",
    "    x = torch.Tensor([2.,2.])\n",
    "    print(x)\n",
    "    y = torch.arange(0,10.0,1).reshape(2,5)\n",
    "    # print(x.transpose(0,1))\n",
    "    # a = torch.normal(0,1,(5,2))\n",
    "    # a = torch.arange(0,1,0.1).reshape(5,2)\n",
    "    print(y)\n",
    "    return torch.matmul(y,x)\n",
    "\n",
    "# 向量和矩阵相乘，矩阵进行转置之后进行mul\n",
    "print(vec_dot_product1())\n",
    "\n",
    "\n",
    "\n",
    "# a = torch.arange(0,10.0,1).reshape(5,2)\n",
    "# print(a)\n",
    "\n",
    "# b = torch.arange(0,10.0,1).reshape(2,5)\n",
    "# print(b)\n",
    "\n",
    "# print(torch.matmul(a,b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、点乘\n",
    "点乘都是broadcast的，可以用torch.mul(a, b)实现，也可以直接用*实现。\n",
    "</BR>注意：广播是补全缺失的列，但是维度必须相同\n",
    "例如：一个二维矩阵，必须和一个二维相乘，两个的列可以不同，系统会自动广播补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "a = torch.ones(3,4)\n",
    "b = torch.Tensor([1,2,3]).reshape((3,1))\n",
    "print(b)\n",
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵乘法\n",
    "\n",
    "如果是二维数组（矩阵）之间的运算，则得到的是矩阵积（mastrix product）。\n",
    "所得到的数组中的每个元素为，第一个矩阵中与该元素行号相同的元素与第二个矩阵与该元素列号相同的元素，两两相乘后再求和。\n",
    "这句话有点难理解，但是这句话里面没有哪个字是多余的。结合下图理解这句话。\n",
    "<BR>\n",
    "mm 只能使用在二维矩阵 matmul可以使用多为矩阵相乘\n",
    "\n",
    "![avatar](numpy.dot.png)\n",
    "<br/>\n",
    "解释：\n",
    "    x = torch.tensor([\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        [2, 2],\n",
    "        [2, 2],\n",
    "        [2, 3]\n",
    "    ])\n",
    "<br/>\n",
    "x 先使用第1行和 y 的第1列相乘，得到新matrix [Update,0][0,0]数据，和y相乘的列决定了当前列的位置，即为0 \n",
    "<br/>\n",
    "x 先使用第1行和 y 的第2列相乘，得到新matrix [0,Update][0,0]数据，和y相乘的列决定了当前列的位置，即为1\n",
    "<br/>\n",
    "之后一次类推；所有两个矩阵相乘需要确认[a,b][c,d] b=c 是否相等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于矩阵乘法的Sample\n",
    "# 如果都是2维，那么就是矩阵乘法的结果返回。与 torch.mm 是等价的，torch.mm 仅仅能处理的是矩阵乘法。\n",
    "\n",
    "# numpy.dot() 对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为内积)；\n",
    "# 对于二维数组，计算的是两个数组的矩阵乘积；对于多维数组，\n",
    "# 它的通用计算公式如下，即结果数组中的每个元素都是：\n",
    "# 数组a的最后一维上的所有元素与数组b的倒数第二位上的所有元素的乘积和： dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])。\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def matrix_multiple():\n",
    "    \n",
    "    x = torch.tensor([\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "    y = torch.tensor([\n",
    "        [2, 2],\n",
    "        [2, 2],\n",
    "        [2, 3]\n",
    "    ])\n",
    "    # print(y.transpose(0,1))\n",
    "    \n",
    "    return torch.matmul(x, y), torch.mm(x, y)\n",
    "\n",
    "print(matrix_multiple())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce539a29b37cbf6656fb94dad7e012f544b2b648a7f96b4739717121bbe2a2ab"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
